# this is ../me/mimi/chores/teaching/current/nlp/class_notes/homework/hw1.org


#+title: Homework 1:  Getting Started and the Structural Units of Language
#+author: Toni Kazic
#+date: Fall, 2024

# revised text of qs 31--34 <2021-08-25 Wed>
#
# revised text of qs 38 (bo-tagger) and 39 <2021-09-14 Tue>


#+SETUPFILE: "../../../common/preamble.org"
#+LATEX_CLASS: article
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil

#+LATEX_HEADER: \usepackage{langsci-avm}
# http://ftp.math.purdue.edu/mirrors/ctan.org/macros/latex/contrib/langsci-avm/langsci-avm.pdf

#+LATEX_HEADER: \newcommand{\grmr}[2]{\ensuremath{\mathrm{#1} & \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\txtgrmr}[2]{\ensuremath{\mathrm{#1} \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\grmrhs}[1]{\ensuremath{& \,\longrightarrow\, \mathrm{#1} }}
#+LATEX_HEADER: \newcommand{\wa}[1]{\type{\textnormal{\w{#1}}}}

# compile with pdflatex
#
# Kazic, 3.11.2020


# fixed question numbering on latex export, at the cost of removing line
# feeds and a little hard-wiring.
#
# It's possible to set the org-empty-line-terminates-plain-lists
#
# Kazic, 31.8.2021
#
# finally, cross-linking on list items!
# https://stackoverflow.com/questions/28151373/orgmode-referring-to-an-item-in-a-numbered-list
#
# Kazic, 1.9.2021


* Introduction

This homework lays the foundation for the course to help you work smoothly
through the semester.  It starts our work on several course objectives and
introduces the basic structural units of languages.  We'll explore some
linguistic and computational approaches to these units.



* Who's Who and Solution Patterns
<<whoswho>>

** Lead Person:  purple


** Group Members

| first name last name | color                                |
|----------------------+--------------------------------------|
| Ethan Glass          | purple \color{violet}\rule{5mm}{3mm} |
| Dillon Jackson       | green \color{green}\rule{5mm}{3mm}   |
| Micheal Hackmann     | yellow \color{yellow}\rule{5mm}{3mm} |



** Three Member Solution Patterns

$i$ is the question number.

#+begin_center
#+ATTR_LaTeX: :mode inline-math :environment array
| \text{color}                  | \text{draft solution} | \text{revise solution} |
|-------------------------------+----------------+-----------------|
| green \color{green}\rule{5mm}{3mm}   | i \mod 3 = 1   | i \mod 3 = 0    |
| yellow \color{yellow}\rule{5mm}{3mm} | i \mod 3 = 2   | i \mod 3 = 1    |
| purple \color{violet}\rule{5mm}{3mm} | i \mod 3 = 0   | i \mod 3 = 2    |
#+end_center


** Two Member Solution Patterns

| color                         | draft solution | revise solution |
|-------------------------------+----------------+-----------------|
| green \color{green}\rule{5mm}{3mm} | odds           | evens           |
| yellow \color{yellow}\rule{5mm}{3mm} | evens          | odds            |




* General Instructions

   + /Fill out the group members table and follow the solution patterns/ in
     Section [[whoswho]].

   + /If the question is unclear, tell me your interpretation of it as part
     of your answer./  Feel free to ask about the questions in class or on
     the Slack channel (use =@channel= as others will probably be puzzled
     too). 

   + /For questions using corpora, use the corpus of the lead person./

   + /Put your draft answers right after each question using a *complete,
     functional* =org= mode code or example block./ Make sure your code
     block is complete and functional by testing it in your copy of this
     homework file.

   + /Each group member reviews the others' draft solutions and you revise them together/.

   + /Discuss each other's draft and reviews, finalizing the answers./

   + /Show all your work: code, results, and analysis./  Does your code
     work in this file and produce *exactly* the results you show? 

   + /Post the completed file to Canvas no later than noon on the Tuesday
     indicated/ in the [[../syllabus.org::schedule][schedule in the syllabus]], naming your file with each
     person's first name (no spaces in the file name, and don't forget the
     =.org= extension!).  Only one person should submit the final file.


   
* Hints

** Follow the instructions in [[file:../notes.org::tech][the technical setup section of the notes]].
Briefly: install [[https://www.gnu.org/software/emacs/download.html][emacs]] for your operating systems; take its tutorial;
install [[file:../../../common/mechanics/pythonesque.org][python and nltk (including the data)]]; practice the examples in
[[file:../mechanics/python_org_mode.org][python_org_mode.org]]; and explore the [[http://www.nltk.org/nltk_data/][various corpora available]].


** *Don't use :session in your code block header!* 
It makes the code blocks interdependent, and I don't want to make a mistake
when cutting and pasting your code to test it.  I should be able to
reproduce your results exactly by running your code block in =org=, so
if it doesn't run you're headed for toast.


** Don't overthink this!

The point of using =NLTK= is to learn it, not re-invent it unless there is a
very good reason.  If you mistrust an answer it gives you, then coding from
scratch and /comparing the results as part of your answer/ is good.


This also applies to =scikit-learn=!




** Make sure you get the right version of the book and documentation when googling.

There are many examples drawn from the first edition of the book and NLTK
2.0 out on the web.  Syntax and functionality have changed between NLTK 2.0
and 3.0:  here's [[https://streamhacker.com/2014/12/02/nltk-3/][a short rundown on these]].



** Some Variations on Loading Python Modules

+ If you just say "import nltk", python doesn't enter the names of any of
  the functions contained within the nltk module into the current symbol
  table.  So when you want to call a function, you have to use the dot
  notation:
#+BEGIN_EXAMPLE
nltk.FCN_NAME()
#+END_EXAMPLE






+ You can import specific functions into the current symbol table:
#+BEGIN_EXAMPLE
from nltk import FCN
#+END_EXAMPLE


And when you do this, they can be called directly:
#+BEGIN_EXAMPLE
FCN()
#+END_EXAMPLE




+ Or, you can import all of the functions at once:
#+BEGIN_EXAMPLE
from nltk import *
#+END_EXAMPLE


and then call them directly:
#+BEGIN_EXAMPLE
FCN()
#+END_EXAMPLE


Importing all the functions in a package or module is generally frowned
upon by the Pythonistas as one wouldn't necessarily know all the names of the 
functions in a module, and they don't want python to confuse the symbols.  While
you can see the current symbol table by typing 
#+BEGIN_EXAMPLE
dir()
#
# or, that for a particular package,
#
dir(nltk)
#+END_EXAMPLE
you might want to play it safe.



+ Finally, you might want to abbreviate a function's (or submodule's) name when you import it:
#+BEGIN_EXAMPLE
import matplotlib.pyplot as plt
. . .
plt.savefig()
#+END_EXAMPLE



** What Can I Do with This Data Structure?

Python implements many data structures, like integers (int), lists ([]),
tuples (()), sets ({}), and dictionaries (aka associative arrays or
key-value lists).  To quickly see what built-in functions are available
for a data structure and get a little help on them:

#+begin_src python
#
# define an empty list
#
L = []
dir(L)

t = ()
dir(t)

help(t.count)

# type q to get back to the python prompt

#+end_src








** If you don't see any output with #+begin_src python :results output, try a print()




** defaultdict is an essentially empty data structure!

For example, it's *not* a dictionary of words and tags.



** Get the right input for the frequency distributions.

In a conditional frequency distribution built from a tagged corpus, the
keys are the tokens and the values are the tags.  NLTK calls the keys
/conditions/: for example, the condition for the value = 'AT' is that the
key = 'the'.  That's very different from the unconditioned frequency
distribution.




** Useful Web Sites

You may find the [[https://docs.python.org/3/tutorial/index.html][official python tutorial]] useful, especially the earlier sections (I did).

[[https://docs.python.org/3/faq/index.html][A collection of Python FAQs]].

[[http://www.nltk.org/py-modindex.html][Index to the current versions of NLTK modules and functions.]]  The examples
in the book may not always be current with the state of the code.


[[https://en.wikipedia.org/wiki/English_prefix][Wikipedia has a fairly thorough list]] of prefixes, but let's use the list of the most
common prefixes found at [[http://dictionary.cambridge.org/us/grammar/british-grammar/prefixes][the Cambridge English Grammar]] site:

#+name: prefixes
#+begin_src python :results output
prefixes = ['anti','auto','de','dis','down','extra','hyper','il','im','in','ir','inter',
             'mega','mid','mis','non','over','out','post','pre','pro','re','semi','sub',
             'super','tele','trans','ultra','un','under','up']
#+end_src







* Questions 

# revised <2021-10-14 Thu> to eliminate freebie



** What are Your GitHub Handles and Corpus Preferences?  Fill Out Here *and DM me the answers.*

1. [@1] If you don't already have one, get a [[https://github.com/][GitHub handle]] and DM this table to
   me on Slack.

| first name | color                                | GitHub handle |
|------------+--------------------------------------+---------------|
|Dillon      | green \color{green}\rule{5mm}{3mm}   |Dailylulll     |
|Micheal     | yellow \color{yellow}\rule{5mm}{3mm} |MichealHackmann  |
|Ethan       | purple \color{violet}\rule{5mm}{3mm} |EthanAGlass    |

For each person in your team, please list in order of decreasing preference
your top three choices for corpora.  Be careful to choose a corpus, not a
model, lexicon, or other lexical aid.  Suggestion: load interesting corpora
and get a few sentences from each.

Please DM me the filled out table on Slack by our third class so I can
resolve any conflicts!

| first name | first choice | second choice | third choice |
|------------+--------------+---------------+--------------|
|Ethan       |Brown         |Twitter        |Gutenberg     |
|Dillon      |Switchboad    |Twitter        |Chat          |
|Micheal     |Genesis       |Shakesphere    |Twitter       |




** 21 emacs questions

# see
# https://stackoverflow.com/questions/28351465/emacs-orgmode-do-not-insert-line-between-headers

Answer the emacs questions giving the KEYSTROKES, following the emacs
conventions for the control and meta keys.  Some questions require answers
in English: stick those in an example block too.


2. [@2] How do you start emacs from the command line?

The keystroke: emacs

3. How do you open a file?

C-x C-f (control x and control f)

4. How do you edit it?



5. How do you save it?

C-x C-s (control x and control s)

6. How do you get help without googling?


C-h f (Control h then f) help with function
C-h k (Control h then k) help with keybinding
C-h t (Control h then t) Opens the Emacs tutorial
C-h ? (Control h then ?) Opens the general help menu


7. How do you get out of trouble?

   If your in a situation where a command is running and is not
   terminating, C-g should cancel out of it.
# second clause added <2021-10-12 Tue>
#
8.  How do you split the window in half horizontally, so that one half is above the other?

   C-x 2
# second clause added <2021-10-12 Tue>
#
9.  How do you split the window in half vertically, so that the halves are side by side?


C-x 3 (Control x then 3)


10.  How long can you repeat the operations in questions 8 and 9?

11.  What is a buffer?

    Memory where text can be manipulated and stored, without being
    automatically saved. Buffers have to be saved before their changes are
    committed permanently.
    
12.  How do you get a list of all the buffers running in your emacs
     process?


C-x C-b (Control x and Control b)

13.  How do you jump to the top of the file without scrolling?

14.  How do you jump to the bottom of the file without scrolling?

    M->

15.  How do you move down a page without scrolling?


C-v (Control c then v)


16.  How do you move up a page without scrolling?

17.  How do you move to the end of a line?

    C-e
    
18.  How do you move to the beginning of a line?


C-a (Control a)


19.  What is the point?


This is the position of the cursor currently in the buffer. This is where you are actively editing. 


20.  What is the mark?

A point in the text when together with the point will define a region between the two. You can set a mark
at the begenning or end of the text in which you want to change. 


# revised to specify mark and point, <2022-10-12 Wed>    
21.  Why are the mark and point useful?

They are extremely important in terms of navifation. You can easily jump to the mark's position by C-x C-x. This will change positions of the point and the mark.
It also makes it easy to select text, which is helpful when trying to copy and paste, or remove large chunks of text. 


22.  How do you exit emacs?

Answer the remaining questions with a corpus of your choice  from the NLTK book.


** 6 python/nltk questions


23.  [@23] What is the command to insert a python code block template?

    < y tab

24.  Load nltk and import the corpus using a python code block.

#+BEGIN_SRC python

  import nltk
  import certifi
  #Certifi and ssl imports were needed to allow for https downloading on my system, they do not affect results of overall code
  import ssl
  from nltk.corpus import twitter_samples

  ssl._create_default_https_context = ssl._create_unverified_context
  #For local security on my Mac, this line is needed to allow for https downloading

  nltk.download('twitter_samples')

#+END_SRC

#+RESULTS:
: None
The result is that the package is downloaded, in our case, twitter_samples.
The package is up to date in my system, but for some people it may be downloaded.
If the result is good, the babel evaluation will exit with code 0. 

25.  How many unique tokens are in your corpus?

From Python script: 42,526

26.  Print out the first 1000 tokens in your corpus.

     #+BEGIN_SRC python :results output
      from nltk.tokenize import word_tokenize
      from nltk.corpus import twitter_samples as twit

      #print(twit.fileids())

      tokens = [];

      for file_id in twit.fileids():
	# load raw text
        text = twit.raw(file_id)

	# tokenize the text
	tokens.extend(word_tokenize(text))

	# add tokens to set of unique tokens
	if len(tokens) >=1000: break
	 
      print(' '.join(tokens[:1000]))
      
    #+END_SRC

    #+results:
    : { `` contributors '' : null , `` coordinates '' : null , `` text '' : `` hopeless for tmr : ( `` , `` user '' : { `` screen_name '' : `` yuwraxkim '' , `` time_zone '' : `` Jakarta '' , `` profile_background_image_url '' : `` http : //pbs.twimg.com/profile_background_images/585476378365014016/j1mvQu3c.png '' , `` profile_background_image_url_https '' : `` https : //pbs.twimg.com/profile_background_images/585476378365014016/j1mvQu3c.png '' , `` default_profile_image '' : false , `` url '' : null , `` profile_text_color '' : `` 000000 '' , `` following '' : false , `` listed_count '' : 3 , `` entities '' : { `` description '' : { `` urls '' : [ ] } } , `` utc_offset '' : 25200 , `` profile_sidebar_border_color '' : `` 000000 '' , `` name '' : `` yuwra \u2708 `` , `` favourites_count '' : 196 , `` followers_count '' : 1281 , `` location '' : `` wearegsd ; favor ; pucukfams ; barbx '' , `` protected '' : false , `` notifications '' : false , `` profile_image_url_https '' : `` https : //pbs.twimg.com/profile_images/622631732399898624/kmYsX_k1_normal.jpg '' , `` profile_use_background_image '' : true , `` profile_image_url '' : `` http : //pbs.twimg.com/profile_images/622631732399898624/kmYsX_k1_normal.jpg '' , `` lang '' : `` id '' , `` statuses_count '' : 19710 , `` friends_count '' : 1264 , `` profile_banner_url '' : `` https : //pbs.twimg.com/profile_banners/3078803375/1433287528 '' , `` geo_enabled '' : true , `` is_translator '' : false , `` contributors_enabled '' : false , `` profile_sidebar_fill_color '' : `` 000000 '' , `` created_at '' : `` Sun Mar 08 05:43:40 +0000 2015 '' , `` verified '' : false , `` profile_link_color '' : `` 000000 '' , `` is_translation_enabled '' : false , `` has_extended_profile '' : false , `` id_str '' : `` 3078803375 '' , `` follow_request_sent '' : false , `` profile_background_color '' : `` 000000 '' , `` default_profile '' : false , `` profile_background_tile '' : true , `` id '' : 3078803375 , `` description '' : `` \u21e8 [ V ] TravelGency \u2588 2/4 Goddest from Girls Day \u2588 92L \u2588 sucrp '' } , `` retweet_count '' : 0 , `` favorited '' : false , `` entities '' : { `` hashtags '' : [ ] , `` user_mentions '' : [ ] , `` urls '' : [ ] , `` symbols '' : [ ] } , `` source '' : `` < a href=\ '' https : //mobile.twitter.com\ '' rel=\ '' nofollow\ '' > Mobile Web ( M2 ) < /a > '' , `` truncated '' : false , `` geo '' : null , `` in_reply_to_status_id_str '' : null , `` is_quote_status '' : false , `` in_reply_to_user_id_str '' : null , `` place '' : null , `` in_reply_to_status_id '' : null , `` in_reply_to_screen_name '' : null , `` lang '' : `` en '' , `` retweeted '' : false , `` in_reply_to_user_id '' : null , `` created_at '' : `` Fri Jul 24 10:42:49 +0000 2015 '' , `` metadata '' : { `` iso_language_code '' : `` en '' , `` result_type '' : `` recent '' } , `` favorite_count '' : 0 , `` id_str '' : `` 624530164626534400 '' , `` id '' : 624530164626534400 } { `` contributors '' : null , `` coordinates '' : null , `` text '' : `` Everything in the kids section of IKEA is so cute . Shame I 'm nearly 19 in 2 months : ( `` , `` user '' : { `` screen_name '' : `` EveHollyHousley '' , `` time_zone '' : `` London '' , `` profile_background_image_url '' : `` http : //pbs.twimg.com/profile_background_images/776873880/f89d8aa869414e41eefd804284a3d95c.jpeg '' , `` profile_background_image_url_https '' : `` https : //pbs.twimg.com/profile_background_images/776873880/f89d8aa869414e41eefd804284a3d95c.jpeg '' , `` default_profile_image '' : false , `` url '' : null , `` profile_text_color '' : `` 333333 '' , `` following '' : false , `` listed_count '' : 0 , `` entities '' : { `` description '' : { `` urls '' : [ ] } } , `` utc_offset '' : 3600 , `` profile_sidebar_border_color '' : `` FFFFFF '' , `` name '' : `` Eve '' , `` favourites_count '' : 4759 , `` followers_count '' : 450 , `` location '' : `` Manchester '' , `` protected '' : false , `` notifications '' : false , `` profile_image_url_https '' : `` https : //pbs.twimg.com/profile_images/620302844093181952/M7IP4ZHa_normal.jpg '' , `` profile_use_background_image '' : false , `` profile_image_url '' : `` http : //pbs.twimg.com/profile_images/620302844093181952/M7IP4ZHa_normal.jpg '' , `` lang '' : `` en '' , `` statuses_count '' : 4384 , `` friends_count '' : 541 , `` profile_banner_url '' : `` https : //pbs.twimg.com/profile_banners/383849833/1435267039 '' , `` geo_enabled '' : true , `` is_translator '' : false , `` contributors_enabled '' : false , `` profile_sidebar_fill_color '' : `` DDEEF6 '' , `` created_at '' : `` Sun Oct 02 16:42:30 +0000 2011 '' , `` verified '' : false , `` profile_link_color '' : `` 0084B4 '' , `` is_translation_enabled '' : false , `` has_extended_profile '' : false , `` id_str '' : `` 383849833 '' , `` follow_request_sent '' : false , `` profile_background_color '' : `` C0DEED '' , `` default_profile '' : false , `` profile_background_tile '' : true , `` id '' : 383849833 , `` description '' : `` Lauv it \u2022 18 \u2022 Music Student '' } , `` retweet_count '' : 0 , `` favorited '' : false , `` entities '' : { `` hashtags '' : [ ] , `` user_mentions '' : [ ] , `` urls '' : [ ] , `` symbols '' : [ ] } , `` source '' : `` < a href=\ '' http : //twitter.com/download/iphone\ '' rel=\ '' nofollow\ '' > Twitter for iPhone < /a > '' , `` truncated '' : false ,


27.  Why do the tokens include punctuation?


- Informal text, tweets in this example use punctuation to convey emotion. For example, using many exclimation points
  could emphasise strong emotion. The use of ":" could be used to combine punctuation to express setniment.

- The use of periods, commas, and question marks are probably used to define the structure of the sentences. They can change
  the meaning of a sentence greatly. The phrase "Let's go Ethan" vs. "Let's go, Ethan" could be the difference in someone
  encouraging me, against someone calling me to go somewhere.

- Punctuation seems to be used to recoginze contractions or abbreivations. Mr. Mrs. Ms. Dr. are some examples of that.

- Punctuation could be used to find sentece boundaries. Segmenting text is important because it can help split sentences,
  or help with syntactic parsing which helps people to understand textual structure. 


28.  How many unique tokens are in the first 1000?




** 6 morpheme/morphosyntax/N-gram questions


29.  [@29] How many of each of the principal modal verbs occur in your corpus?

    #+BEGIN_SRC python :results output
      from nltk.tokenize import word_tokenize
      from nltk.corpus import twitter_samples as twit
      from collections import Counter

      # define the principal modal verbs
      modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would']

      # initialize counter to hold the counts
      modal_counts = Counter()

      # process each file in the twitter corpus
      for file_id in twit.fileids():
	# load raw text
	text = twit.raw(file_id)

	# tokenize the text
	tokens = word_tokenize(text)

	# count modal verbs
	for token in tokens:
	  if token.lower() in modal_verbs:
	    modal_counts[token.lower()] += 1

      # print the counts of each modal verb
      output = "\n".join(f"{modal}: {count}" for modal, count in modal_counts.items())
      print(output)
    #+END_SRC

    #+results:
    : may: 4733
    : can: 2280
    : will: 4889
    : might: 284
    : would: 2711
    : could: 839
    : should: 596
    : shall: 80
    
30.  How many unique bigrams are in your text?

#+BEGIN_SRC python :results output
import nltk
from nltk.corpus import twitter_samples
from nltk import bigrams as bg
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

nltk.download('twitter_samples')

data_files = twitter_samples.fileids()

all_words = []
for f in data_files:
    all_words += twitter_samples.tokenized(f)

list_of_words = [w for tweet in all_words for w in tweet]

word_pairs = list(bg(list_of_words))

u_pairs = set(word_pairs)
total_pairs = len(u_pairs)

print(f"Unique bigram count is: {total_pairs}")

#+END_SRC

#+RESULTS:
: Unique bigram count is: 175759


# stopword caution
31.  How many bigrams contain the modal \w{can}?  Compute both for all and the
    unique bigrams.  Don't exclude stopwords!

# revised to insist on a prefix
#
# added link and pointer to the list of prefixes, fall 2023
#
# <<pref-regex>>
32.  <<pref-regex>> Choose a prefix from the [[prefixes][list above]] (line 288) and
    write a regular expression that identifies *all* words in your corpus
    containing that prefix (remember that a prefix begins the word).
    What's the length of that group of words?

    #+BEGIN_SRC python :results output
      import re
      from nltk.corpus import twitter_samples as twit

      # regex pattern
      pattern = r'\bhyper\w*'

      matches = []

      # loop through twitter files
      for file_id in twit.fileids():
	text = twit.raw(file_id)

	# find matches for pattern
	found_matches = re.findall(pattern, text, re.IGNORECASE)

	# add matches to matches set
	matches.extend(found_matches)

      # cut out duplicate matches
      unique_matches = set(matches)

      for word in sorted(unique_matches):
	print(word)

      # print number of words
      print('Number of words with prefix "hyper": ' + str(len(unique_matches)))
    #+END_SRC

    #+results:
    #+begin_example
    HYPERSTATlON
    Hyper
    Hyper67
    HyperDruid
    hyper
    hyperbole
    hyperbullies
    hypercholesteloremia
    hyperdruid
    hypernumbers
    Number of words with prefix "hyper": 10
    #+end_example

# revised to specify what to show
#
# clarified it's the set of words thats meant <2023-10-20 Fri>
#
# pref-regex
33.  <<sort-set>> Sort the group of words in problem [[pref-regex]] in alphabetical order (show your code and
    results) and study it.  Do you see multiple forms of the same headword?
    Show some examples.

#+BEGIN_SRC python :results output
  import nltk
  from nltk.corpus import twitter_samples
  import re
  from collections import defaultdict
  import ssl

  # Ensure SSL is properly configured for downloading
  ssl._create_default_https_context = ssl._create_unverified_context

  nltk.download('twitter_samples')

  file_ids = twitter_samples.fileids()

  all_words = []
  for f in file_ids:
      all_words += twitter_samples.tokenized(f)

  words_list = [word for tweet in all_words for word in tweet]

  prefix = "re"
  pattern = rf'^{prefix}.*'  

  #Matched words will find all of the words with the user given prefix
  matched_words = [word for word in words_list if re.match(pattern, word)]

  #This will get the length of the array, get rid of the duplicates and form a set
  unique_matched_words = set(matched_words)

  #Built in, Python sorted function will arrange all of the words in alphabetical order for easy reading after print
  sorted_words = sorted(unique_matched_words)

  print("Here is an array of Sorted Words with the Prefix 're':")
  print(sorted_words)

  #This will act as a variable that combines and identifies forms with the same head word. 
  the_headwords = defaultdict(list)

  #Acts to group words by their headwords, without suffixes
  for w in sorted_words:
      condense_word = re.sub(r'(ing|s|ly|es|ful|is|de|ness)$', '', w)
      the_headwords[condense_word].append(w)

  print("\nHeadwords and Their Forms:")
  for headword, forms in the_headwords.items():
      if len(forms) > 1:
	 print(f"{headword}: {', '.join(forms)}")
#+END_SRC

#+RESULTS:
#+begin_example
Here is an array of Sorted Words with the Prefix 're':
['re', 're-arranging', 're-cap', 're-hash', 're-hiring', 're-run', 're-sit', 're-standing', 're-think', 're-watch', 'reaally', 'reaaly', 'reaapearing', 'reach', 'reached', 'reaches', 'reaching', 'react', 'reacted', 'reaction', 'reactionary', 'reactions', 'reactivates', 'read', 'reader', 'readers', 'reading', 'ready', 'reaffirm', 'real', 'realise', 'realised', 'realises', 'realising', 'realistic', 'realisticly', 'reality', 'reality-based', 'realize', 'realized', 'reallY', 'really', 'realness', 'realy', 'rear', 'rear-ended', 'reason', 'reasonable', 'reasonably', 'reasoning', 'reasons', 'reassured', 'rebel', 'reboarding', 'rebound', 'rebounds', 'rebuttal', 'rec', 'recall', 'recap', 'receive', 'received', 'receives', 'receiving', 'recent', 'recently', 'receptionist', 'receptionists', 'recession', 'rechargeable', 'recieved', 'recipe', 'recipes', 'reciprocal', 'recitation', 'recites', 'reciting', 'reckon', 'reckons', 'reco', 'recognise', 'recognising', 'recognition', 'recognized', 'recommend', 'recommendation', 'recommendations', 'recommended', 'record', 'recorded', 'recording', 'recordings', 'records', 'recover', 'recovered', 'recovering', 'recovery', 'recreate', 'recruited', 'recruitment', 'recs', 'recuerda', 'recveives', 'red', 'redecorate', 'redeem', 'redid', 'redirected', 'redistribute', 'redistribution', 'reduce', 'reduced', 'reducing', 'redundancy', 'redundant', 'reece', 'reeeeally', 'reeling', 'reevaluate', 'ref', 'refer', 'refere', 'references', 'referend', 'referendum', 'referred', 'referring', 'refinements', 'reflect', 'reflected', 'reflection', 'refollow', 'reform', 'reformers', 'reforms', 'refrain', 'refresh', 'refreshed', 'refreshing', 'refreshingly', 'refugees', 'refur', 'refurbished', 'refusal', 'refuse', 'refused', 'refuses', 'refusing', 'refute', 'refuted', 'regain', 'regard', 'regarding', 'regardless', 'regens', 'reggae', 'regime', 'region', 'regional', 'regions', 'regressive', 'regret', 'regrets', 'regrettably', 'regretted', 'regular', 'regularly', 'regulate', 'regulated', 'regulation', 'regulations', 'regulatory', 'reh', 'rehash', 'rehearsal', 'rehearse', 'rehearsed', 'reheat', 'reich', 'reigned', 'reinforce', 'reinforces', 'reinvents', 'reject', 'rejected', 'rejecting', 'rejection', 'rejects', 'rela', 'relapse', 'relate', 'related', 'relating', 'relation', 'relationship', 'relationships', 'relative', 'relatively', 'relatives', 'relativists', 'relax', 'relaxed', 'relaxes', 'relaxing', 'release', 'released', 'releasing', 'relegation', 'relented', 'relentlessly', 'relevant', 'reliable', 'reliant', 'relief', 'relies', 'reliever', 'religio', 'religion', 'religions', 'relive', 'relojes', 'reluctant', 'rely', 'relying', 'remain', 'remained', 'remaining', 'remains', 'remark', 'remarkable', 'remarks', 'remastered', 'reme', 'remedies', 'remedy', 'remember', 'rememberable', 'remembered', 'remembers', 'remind', 'reminded', 'reminder', 'reminders', 'reminding', 'reminds', 'reminiscent', 'remix', 'remixes', 'remote', 'remove', 'removed', 'removing', 'renamed', 'reneging', 'renegotiate', 'renegotiating', 'renew', 'renewal', 'renewing', 'rent', 'renunciar√°', 'reorganisation', 'rep', 'repack', 'repackage', 'repacks', 'repair', 'repairs', 'repath', 'repeal', 'repealed', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repects', 'repellant', 'repetition', 'replace', 'replaced', 'replacement', 'replacements', 'replaces', 'replay', 'replayed', 'replicate', 'replied', 'replies', 'reply', 'replying', 'report', 'reported', 'reporter', 'reporters', 'reporting', 'reports', 'reppin', 'represent', 'representation', 'representations', 'representative', 'representatives', 'represented', 'representing', 'represents', 'repression', 'reps', 'republican', 'republicanism', 'repugnant', 'reputation', 'reqd', 'request', 'requesting', 'requests', 'require', 'required', 'requirements', 'requires', 'reread', 'rescinded', 'rescoops', 'rescue', 'rescuing', 'research', 'researchers', 'researching', 'resemblance', 'reservation', 'reservations', 'reservations@sandsbeach.eu', 'reserve', 'reset', 'residents', 'resign', 'resignation', 'resignations', 'resigns', 'resin', 'resist', 'resonance', 'resort', 'resorting', 'resorts', 'resource', 'resources', 'respect', 'respectable', 'respected', 'respecting', 'respective', 'respects', 'respond', 'responde', 'responded', 'responder', 'responds', 'respons', 'response', 'responses', 'responsibilities', 'responsibility', 'responsibilty', 'responsible', 'responsiblity', 'respues', 'rest', 'restarted', 'restaurants', 'resting', 'restore', 'restrained', 'restrict', 'restricted', 'rests', 'result', 'resultUKIP', 'resultin', 'resulting', 'results', 'resy', 'retail', 'retailers', 'retain', 'retainers', 'retaining', 'retaliate', 'retard', 'retarded', 'reticence', 'reticent', 'retire', 'retired', 'retirement', 'retriever', 'return', 'returned', 'returning', 'returns', 'retweet', 'retweeted', 'retweeting', 'retweets', 'reunite', 'reunited', 'reusanta', 'reusful', 'revamps', 'reveal', 'revealed', 'reveals', 'revelation', 'revenge', 'revenue', 'revenues', 'reverse', 'reversing', 'review', 'reviewing', 'reviews', 'revise', 'revival', 'revolution', 'revolving', 'reward', 'rewarding', 'rewatched', 'rewatches', 'rewert', 'rewind', 'reworded', 'rewording', 'rewrite', 'reynoldsgrl']

Headwords and Their Forms:
re: re, rely
reach: reach, reaches, reaching
reaction: reaction, reactions
read: read, reading
reader: reader, readers
real: real, really, realness
realis: realises, realising
realistic: realistic, realisticly
reason: reason, reasoning, reasons
rebound: rebound, rebounds
rec: rec, recs
receiv: receives, receiving
recent: recent, recently
receptionist: receptionist, receptionists
recit: recites, reciting
reckon: reckon, reckons
recommendation: recommendation, recommendations
record: record, recording, records
recover: recover, recovering
reform: reform, reforms
refresh: refresh, refreshing
refus: refuses, refusing
regard: regard, regarding
region: region, regions
regret: regret, regrets
regular: regular, regularly
regulation: regulation, regulations
reject: reject, rejecting, rejects
relationship: relationship, relationships
relative: relative, relatively
relax: relax, relaxes, relaxing
religion: religion, religions
remain: remain, remaining, remains
remark: remark, remarks
remember: remember, remembers
remind: remind, reminding, reminds
reminder: reminder, reminders
remix: remix, remixes
renew: renew, renewing
rep: rep, reply, reps
repack: repack, repacks
repair: repair, repairs
repeat: repeat, repeating, repeats
repeated: repeated, repeatedly
replacement: replacement, replacements
report: report, reporting, reports
reporter: reporter, reporters
represent: represent, representing, represents
representation: representation, representations
request: request, requesting, requests
research: research, researching
reservation: reservation, reservations
resign: resign, resigns
resignation: resignation, resignations
resort: resort, resorting, resorts
respect: respect, respecting, respects
respond: respond, responds
respon: responde, respons
rest: rest, resting, rests
result: result, resulting, results
retain: retain, retaining
return: return, returning, returns
retweet: retweet, retweeting, retweets
reveal: reveal, reveals
review: review, reviewing, reviews
reward: reward, rewarding
#+end_example

We see frequent examples of froms with the same headword. In this example, I used the prefix 're' which is a farily common
one that would draw plenty of examples. This could be changed in the code to give exmaples of other forms. By getting matched words and then
sorting them we can get a list of the words with the same prefix. Then once we group words by their headword, and then remove anything that could affect
the suffixes, some of the examples are:
- rebuilt
  rebuild
  remove
  removes
  removed
  removing

In this list of exmaples alone, you can see the variation by choosing the same headword. 


34.  Now use the Porter stemmer on the group of words from problem [[sort-set]] (again, show
    your code and results).  What do you notice?



** 8 Tags, Tagsets, and Tagging

35.  [@35] Ask NLTK what tags the Penn Treebank tagset uses for nouns and verbs
    (and show the result).

    #+BEGIN_SRC python :results output
      from nltk.corpus import treebank as tb

      #tagset = tb._tagset
      tagset = set(tag for word, tag in tb.tagged_words())

      noun_tags = {tag for tag in tagset if tag.startswith('N')}
      verb_tags = {tag for tag in tagset if tag.startswith('V')}

      print("Nouns: ")
      print(noun_tags)
      print("Verbs: ")
      print(verb_tags)


    #+END_SRC

    #+results:
    : Nouns: 
    : {'NNS', 'NNP', 'NN', 'NNPS'}
    : Verbs: 
    : {'VBN', 'VBP', 'VB', 'VBG', 'VBZ', 'VBD'}

#  <<ran-sent>> 
36. <<ran-sent>> Import the corpus and count the number of sentences in it, discarding
    any uninteresting front matter such as titles and chapter headings.
    Then show a random sentence.

#+BEGIN_SRC python :results output
  import nltk
  from nltk.corpus import twitter_samples
  import ssl
  import random
  import re

  ssl._create_default_https_context = ssl._create_unverified_context

  nltk.download('twitter_samples')
  file_ids = twitter_samples.fileids()

  # loading all tweets from files
  tweets = []
  for id in file_ids:
      tweets += twitter_samples.strings(id)

  #Putting all of the tweets together before spliitting
  all_tweets = " ".join(tweets)

  #Split the tweets into sentences
  #remove empty strings, and split by punctuation. 
  sent = re.split(r'\.\s+|!\s+|\.\.\.\s*', all_tweets)

  sent = [s.strip() for s in sent if s]

  total_sent = len(sent)

  rand_sent = random.choice(sent)

  #This will clean up the sentence to avoid words with @ as the prefix and will make it empty. Similar to a title in my eyes, most sentences starting with RT: don't work well. 
  sanitized_sent = re.sub(r'@\w+\b', '', rand_sent).strip()

  sanitized_sent = re.sub(r'RT\s*:\s*.*', '', sanitized_sent).strip()

  print(f"The number of sentences in the corpus appears to be: {total_sent}")
  print(f"The random sentence retracted from the Twitter Corpus is: {sanitized_sent}")

  #+END_SRC

  #+RESULTS:
  : The number of sentences in the corpus appears to be: 20567
  : The random sentence retracted from the Twitter Corpus is: #VoteSNP #SNP Yeah, that is pretty messed up
  

  # added explicit instruction to use a tagger <2023-10-20 Fri>
  37.  Use a tagger each token in your random sentence as 'VB'.

#+begin_src python :results output
  import nltk
  from nltk.corpus import twitter_samples
  import ssl
  import random
  import re
  from nltk import word_tokenize
  from nltk import DefaultTagger

  ssl._create_default_https_context = ssl._create_unverified_context

  nltk.download('twitter_samples')
  file_ids = twitter_samples.fileids()

  # loading all tweets from files
  tweets = []
  for id in file_ids:
      tweets += twitter_samples.strings(id)

  #Putting all of the tweets together before spliitting
  all_tweets = " ".join(tweets)

  #Split the tweets into sentences
  #remove empty strings, and split by punctuation. 
  sent = re.split(r'\.\s+|!\s+|\.\.\.\s*', all_tweets)

  sent = [s.strip() for s in sent if s]

  total_sent = len(sent)

  rand_sent = random.choice(sent)

  #This will clean up the sentence to avoid words with @ as the prefix and will make it empty. Similar to a title in my eyes, most sentences starting with RT: don't work well. 
  sanitized_sent = re.sub(r'@\w+\b', '', rand_sent).strip()


  sanitized_sent = re.sub(r'RT\s*:\s*.*', '', sanitized_sent).strip()
  tokens = word_tokenize(sanitized_sent)
  default_tagger = DefaultTagger('VB')
  taggs = default_tagger.tag(tokens)
  print("\n")
  print(taggs)
#+end_src

#+results:
: 
: 
: [('I', 'VB'), ('do', 'VB'), ("n't", 'VB'), ('mind', 'VB'), (':', 'VB'), ('p', 'VB'), ('ahahha', 'VB'), ('Chris', 'VB'), (',', 'VB'), ('that', 'VB'), ("'s", 'VB'), ('great', 'VB'), ('to', 'VB'), ('hear', 'VB'), (':', 'VB'), (')', 'VB'), ('Due', 'VB'), ('times', 'VB'), ('&', 'VB'), ('amp', 'VB'), (';', 'VB'), ('reminders', 'VB'), ('are', 'VB'), ('indeed', 'VB'), ('planned', 'VB'), (',', 'VB'), ('both', 'VB'), ('will', 'VB'), ('be', 'VB'), ('available', 'VB'), ('in', 'VB'), ('the', 'VB'), ('not', 'VB'), ('too', 'VB'), ('distant', 'VB'), ('future', 'VB')]
    
  # <<silly-tags>>
  38. <<silly-tags>> Of course that's pretty silly, but we can make it even sillier.  Build
      a dictionary of the incorrectly tagged words in your sentence using the
      following tags: 
#+begin_src python
  silly = ['FOO','BAR','EGO','NEED','ADS','DUCK','MANSE']
#+end_src

    #+begin_src python :results output
      import nltk
      from nltk.corpus import treebank as tb
      from nltk.corpus import twitter_samples
      import ssl
      import random
      import re
      from nltk import ConditionalFreqDist
      from nltk import word_tokenize
      from nltk import DefaultTagger


      silly = ['FOO','BAR','EGO','NEED','ADS','DUCK','MANSE']

      ssl._create_default_https_context = ssl._create_unverified_context

      nltk.download('twitter_samples')
      file_ids = twitter_samples.fileids()

      # loading all tweets from files
      tweets = []
      for id in file_ids:
	tweets += twitter_samples.strings(id)

      #Putting all of the tweets together before spliitting
      all_tweets = " ".join(tweets)

      #Split the tweets into sentences
      #remove empty strings, and split by punctuation. 
      sent = re.split(r'\.\s+|!\s+|\.\.\.\s*', all_tweets)

      sent = [s.strip() for s in sent if s]

      total_sent = len(sent)

      rand_sent = random.choice(sent)

      #This will clean up the sentence to avoid words with @ as the prefix and will make it empty. Similar to a title in my eyes, most sentences starting with RT: don't work well. 
      sanitized_sent = re.sub(r'@\w+\b', '', rand_sent).strip()


      sanitized_sent = re.sub(r'RT\s*:\s*.*', '', sanitized_sent).strip()

      print(sanitized_sent)
      words = sanitized_sent.split()
      pos = {}
      index = 0
      for word in words:
	pos[word] = silly[index]
	index += 1
	if index == 7:
	  index = 0

      print(pos)
    #+end_src

    #+results:
    : How can Carwyn Jones (A LABOUR FM!) work for a man who would rather let the Tories into Downing Street than let the #SNP and #Plaid15 help?
    : {'How': 'FOO', 'can': 'BAR', 'Carwyn': 'EGO', 'Jones': 'NEED', '(A': 'ADS', 'LABOUR': 'DUCK', 'FM!)': 'MANSE', 'work': 'FOO', 'for': 'BAR', 'a': 'EGO', 'man': 'NEED', 'who': 'ADS', 'would': 'DUCK', 'rather': 'MANSE', 'let': 'FOO', 'the': 'BAR', 'Tories': 'EGO', 'into': 'NEED', 'Downing': 'ADS', 'Street': 'DUCK', 'than': 'MANSE', '#SNP': 'EGO', 'and': 'NEED', '#Plaid15': 'ADS', 'help?': 'DUCK'}

#  [[ran-sent]]

39. [@39]  <<bo-tagger>> Construct a lookup tagger trained on the 1000 most frequent words in
    the Brown news category that backs off to a default tag of 'UNK'.  Use
    that to tag the original random sentence you got in question [[ran-sent]].  Print
    the result as the list of tuples in sentence order.  What do you
    observe?

#  ran-sent and bo-tagger
40. [@40] For your random sentence found in question [[ran-sent]] tagged with
    the tagger you built in question [[bo-tagger]], write a
    transformational rule the Brill tagger might discover for the each of
    the first three UNK tags.  Put these in an example block as ordinary
    text, /e.g./:
#+BEGIN_EXAMPLE
tagged sentence
rule i:   context:  X -> Y
#+END_EXAMPLE

# <<to-gram>>
41. [@41]  <<to-gram>> Example 2.2 in [[http://www.nltk.org/book/ch07.html][chapter 7]] shows a little grammar for noun phrase
    chunking.  Let's mix it up a bit and define a grammar for "to phrases":
    bigrams that begin with the tag =TO=.  Show the total parse and just
    the "to phrases" (just edit away the rest unless you feel like getting
    fancy). Use the following sentence to build and test your grammar:
#+begin_src python :results output

# more convenient one-liner
tj = [('He', 'PPS'), ('had', 'HVD'), ('nothing', 'UNK'), ('to', 'TO'), ('urge', 'UNK'), ('against', 'IN'), ('it', 'PPS'), (',', ','), ('but', 'CC'), ('still', 'RB'), ('resisted', 'UNK'), ('the', 'AT'), ('idea', 'UNK'), ('of', 'IN'), ('a', 'AT'), ('letter', 'UNK'), ('of', 'IN'), ('proper', 'UNK'), ('submission', 'UNK'), (';', '.'), ('and', 'CC'), ('therefore', 'UNK'), (',', ','), ('to', 'TO'), ('make', 'VB'), ('it', 'PPS'), ('easier', 'UNK'), ('to', 'TO'), ('him', 'PPO'), (',', ','), ('as', 'CS'), ('he', 'PPS'), ('declared', 'VBD'), ('a', 'AT'), ('much', 'AP'), ('greater', 'UNK'), ('willingness', 'UNK'), ('to', 'TO'), ('make', 'VB'), ('mean', 'UNK'), ('concessions', 'UNK'), ('by', 'IN'), ('word', 'NN'), ('of', 'IN'), ('mouth', 'UNK'), ('than', 'IN'), ('on', 'IN'), ('paper', 'UNK'), (',', ','), ('it', 'PPS'), ('was', 'BEDZ'), ('resolved', 'UNK'), ('that', 'CS'), (',', ','), ('instead', 'UNK'), ('of', 'IN'), ('writing', 'UNK'), ('to', 'TO'), ('Fanny', 'UNK'), (',', ','), ('he', 'PPS'), ('should', 'MD'), ('go', 'VB'), ('to', 'TO'), ('London', 'UNK'), (',', ','), ('and', 'CC'), ('personally', 'UNK'), ('intreat', 'UNK'), ('her', 'PP$'), ('good', 'JJ'), ('offices', 'UNK'), ('in', 'IN'), ('his', 'PP$'), ('favour', 'UNK'), ('.--', 'UNK'), ('"', 'UNK'), ('And', 'CC'), ('if', 'CS'), ('they', 'PPSS'), ('really', 'RB'), ('DO', 'UNK'), ('interest', 'NN'), ('themselves', 'PPLS'), (',"', 'UNK'), ('said', 'VBD'), ('Marianne', 'UNK'), (',', ','), ('in', 'IN'), ('her', 'PP$'), ('new', 'JJ'), ('character', 'UNK'), ('of', 'IN'), ('candour', 'UNK'), (',', ','), ('"', 'UNK'), ('in', 'IN'), ('bringing', 'UNK'), ('about', 'IN'), ('a', 'AT'), ('reconciliation', 'UNK'), (',', ','), ('I', 'PPSS'), ('shall', 'UNK'), ('think', 'VB'), ('that', 'CS'), ('even', 'RB'), ('John', 'NP'), ('and', 'CC'), ('Fanny', 'UNK'), ('are', 'BER'), ('not', '*'), ('entirely', 'UNK'), ('without', 'IN'), ('merit', 'UNK'), ('."', 'UNK')]

# for prettier printing

from nltk.corpus import treebank as tb
from nltk import bigrams
from nltk import ConditionalFreqDist
from nltk import word_tokenize
from nltk import RegexpParser

grammar = "TO_PHRASE: {<TO> <NN|NNS|NNP|NNPS|PPS|PPO|PP$|VB|VBD|VBG|VBN|VBP|VBZ|JJ|RB|IN|CS|MD|AP|UNK|DT>}"
#grammar = "NP: {<DT>?<JJ>*<NN>}"
cp = RegexpParser(grammar) 
result = cp.parse(tj) 
print(result)
  
print("\nTo Phrases:")
for subtree in result.subtrees():
  if subtree.label() == 'TO_PHRASE':
    print(subtree)

#+end_src

#+results:
#+begin_example
(S
  He/PPS
  had/HVD
  nothing/UNK
  (TO_PHRASE to/TO urge/UNK)
  against/IN
  it/PPS
  ,/,
  but/CC
  still/RB
  resisted/UNK
  the/AT
  idea/UNK
  of/IN
  a/AT
  letter/UNK
  of/IN
  proper/UNK
  submission/UNK
  ;/.
  and/CC
  therefore/UNK
  ,/,
  (TO_PHRASE to/TO make/VB)
  it/PPS
  easier/UNK
  (TO_PHRASE to/TO him/PPO)
  ,/,
  as/CS
  he/PPS
  declared/VBD
  a/AT
  much/AP
  greater/UNK
  willingness/UNK
  (TO_PHRASE to/TO make/VB)
  mean/UNK
  concessions/UNK
  by/IN
  word/NN
  of/IN
  mouth/UNK
  than/IN
  on/IN
  paper/UNK
  ,/,
  it/PPS
  was/BEDZ
  resolved/UNK
  that/CS
  ,/,
  instead/UNK
  of/IN
  writing/UNK
  (TO_PHRASE to/TO Fanny/UNK)
  ,/,
  he/PPS
  should/MD
  go/VB
  (TO_PHRASE to/TO London/UNK)
  ,/,
  and/CC
  personally/UNK
  intreat/UNK
  her/PP$
  good/JJ
  offices/UNK
  in/IN
  his/PP$
  favour/UNK
  .--/UNK
  "/UNK
  And/CC
  if/CS
  they/PPSS
  really/RB
  DO/UNK
  interest/NN
  themselves/PPLS
  ,"/UNK
  said/VBD
  Marianne/UNK
  ,/,
  in/IN
  her/PP$
  new/JJ
  character/UNK
  of/IN
  candour/UNK
  ,/,
  "/UNK
  in/IN
  bringing/UNK
  about/IN
  a/AT
  reconciliation/UNK
  ,/,
  I/PPSS
  shall/UNK
  think/VB
  that/CS
  even/RB
  John/NP
  and/CC
  Fanny/UNK
  are/BER
  not/*
  entirely/UNK
  without/IN
  merit/UNK
  ."/UNK)

To Phrases:
(TO_PHRASE to/TO urge/UNK)
(TO_PHRASE to/TO make/VB)
(TO_PHRASE to/TO him/PPO)
(TO_PHRASE to/TO make/VB)
(TO_PHRASE to/TO Fanny/UNK)
(TO_PHRASE to/TO London/UNK)
#+end_example

#  [[to-gram]], 40
42.  [@42] What do you observe in your results for question [[to-gram]]?  Why do you think
    this is happening?

    It looks like everything is working, it's picking out the proper
    bigrams beginning with "TO" and everything is going the way it's
    supposed to. One observation is that it appears that words tagged "UNK"
    are more likely to follow the "TO", so that could be something
    interesting. 


* Grading Scale

# revised <2021-10-14 Thu> to correct typo

This homework is worth 15 points.  Complete answers for question 1, here
and in the Slack channel, are required: otherwise /no/ points will be
awarded.  The grading scale is:

| fraction correctly reviewed and answered | points awarded |
|------------------------------------------+----------------|
| \(\ge 0.95\)                             |             15 |
| 0.90 -- 0.94                             |             14 |
| 0.85 -- 0.89                             |             13 |
| 0.80 -- 0.84                             |             12 |
| 0.75 -- 0.79                             |             11 |
| 0.70 -- 0.74                             |             10 |
| 0.65 -- 0.69                             |              9 |
| 0.60 -- 0.64                             |              8 |
| 0.55 -- 0.59                             |              7 |
| 0.50 -- 0.54                             |              6 |
| 0.45 -- 0.49                             |              5 |
| 0.40 -- 0.44                             |              4 |
| 0.35 -- 0.39                             |              3 |
| 0.30 -- 0.34                             |              2 |
| 0.25 -- 0.29                             |              1 |
| \(< 0.25\)                               |              0 |




* Scoring




|     question | ok? |
|--------------+-----|
|            1 |     |
|            2 |     |
|            3 |     |
|            4 |     |
|            5 |     |
|            6 |     |
|            7 |     |
|            8 |     |
|            9 |     |
|           10 |     |
|           11 |     |
|           12 |     |
|           13 |     |
|           14 |     |
|           15 |     |
|           16 |     |
|           17 |     |
|           18 |     |
|           19 |     |
|           20 |     |
|           21 |     |
|           22 |     |
|           23 |     |
|           24 |     |
|           25 |     |
|           26 |     |
|           27 |     |
|           28 |     |
|           29 |     |
|           30 |     |
|           31 |     |
|           32 |     |
|           33 |     |
|           34 |     |
|           35 |     |
|           36 |     |
|           37 |     |
|           38 |     |
|           39 |     |
|           40 |     |
|           41 |     |
|           42 |     |
|--------------+-----|
|  total score |   0 |
|     fraction |   0 |
| total points |     |
#+TBLFM: @44$2=vsum(@I..@II)::@45$2=@-1/(@-2$1)

